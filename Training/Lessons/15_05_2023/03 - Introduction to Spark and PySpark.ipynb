{"cells":[{"cell_type":"markdown","source":["# Background & motivation"],"metadata":{"id":"sl7Bv3gG-9Xr","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d20da167-5172-458c-a587-ac3c9719c3e1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Big data is a general term referring to several challenges encountered when dealing with a large amount of data. These challenges are known as **the 5 V's** - Volume, Variety, Velocity, Variability and Veracity. Traditionally, the 5 V's were solved by the **scale-up** approach, which means upgrading the resources - better servers, faster connections, larger memories, etc. This is now replaced by the **scale-out** approach, which is based on **parallelizing** tasks over a **cluster** of \"weak\" servers.\n\nIt is important to notice that the 5 V's are **infrastructural** challenges and are not necessarily related to any business or analytical problem. This is important to notice, because the confusion between the two is very common, making people think that data science and big data are similar expertise. \n\nLet's discuss some important terms:\n\n* [**Cluster**][cluster] - a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Any cluster has a **master** (or main) and **nodes** (or slaves), which offer two main advantages - **Fault-tolerance** and **Data locality**. \n* [**MapReduce**][mapreduce] - a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster.\n* [**Hadoop**][hadoop] - an open-source software framework used for distributed storage and processing of big data sets using the MapReduce programming model.\n    * **Hadoop Data File System (HDFS)** - a distributed, scalable, and portable file system for the Hadoop framework.\n    * **Hadoop ecosystem** - collection of additional software packages that can be installed on top of or alongside Hadoop (e.g. Yarn, Pig, Hive, Sqoop, Mahout, etc.). Many of the big data tools aim to **abstract** the parallelization.\n    * **Hadoop distribution** - companies providing Hadoop-based software, support, services and training (e.g. Cloudera, Hortonworks, etc.)\n\n[cluster]: https://en.wikipedia.org/wiki/Computer_cluster \"Computer cluster - Wikipedia\"\n[mapreduce]: https://en.wikipedia.org/wiki/MapReduce \"MapReduce - Wikipedia\"\n[hadoop]: https://en.wikipedia.org/wiki/Apache_Hadoop \"Hadoop - Wikipedia\"\n[hdfs]: https://en.wikipedia.org/wiki/Apache_Hadoop#HDFS \"HDFS - Wikipedia\""],"metadata":{"id":"VSaKTAIK-9Xs","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2256de68-2376-4624-a80e-bacc94ce645b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Spark"],"metadata":{"id":"ITaoh0VeeLUF","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1576d6ab-a6b4-4e5d-82e4-cf9438ea6a69","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Basics"],"metadata":{"id":"kVkK3vCnW56F","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a175fff7-920b-47f8-9cdd-96b786b19ef9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["[Spark](https://spark.apache.org/docs/latest/) is a framework for programming with an abstraction of the map-reduce paradigm. Its main data structure (**RDD**) allows better utilization of the memory of the nodes, and this made it very popular in recent years. Spark was originally part of the Hadoop ecosystem, however it was so useful, that eventually it was decided to make it available as a stand-alone framework. \n\nSpark is made of 5 building blocks:\n\n* Spark core - the fundamentals components of the language. It provides distributed task dispatching, scheduling, and basic I/O functionalities, exposed through an API centered on the RDD abstraction.\n* Spark SQL - tools for working with DataFrames. It provides an API for embedding SQL scripts, as well as connections with an ODBC/JDBC server.\n* Spark streaming - facilitates tasks witha a data stream. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data.\n* Spark MLlib - distributed versions of various machine learning (ML) algorithms.\n* Spark GraphX - graph processing framework."],"metadata":{"id":"aTKW09qoWXH_","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa059653-c955-431f-ac85-ea2dd66b0cc4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["[PySpark](https://spark.apache.org/docs/latest/api/python/index.html) is the official Python API for Spark."],"metadata":{"id":"aEln5bkPm7Nk","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"77a30d37-853b-47a1-8fb1-9a52a0d9bfa5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Working with Spark"],"metadata":{"id":"7HSivblGWXIO","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c98d231-759d-4d01-9b1e-b25ba3984085","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The first thing a Spark program must do is to create a `SparkContext` object (traditionally symbolized as the variable `sc`), which tells Spark how to access a cluster. (To create a `SparkContext` you might first need to build a `SparkConf` object that contains information about your application.)."],"metadata":{"id":"ttaS81aGWXIR","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8f4b2707-88e9-4dc0-b09c-d283e42ca90f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Spark is written in [Scala](https://en.wikipedia.org/wiki/Scala_(programming_language)), but it suports APIs for Java, R and Python. We will work with the official Python API - [**PySpark**](https://spark.apache.org/docs/latest/api/python/index.html)."],"metadata":{"id":"YZaCYcXxFTJ2","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f658b9ab-04e2-4c59-9e66-9095153ef2ff","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# from pyspark import SparkContext\n# sc = SparkContext.getOrCreate()"],"metadata":{"id":"OUEnSMnd2K0P","colab":{"base_uri":"https://localhost:8080/","height":322},"outputId":"c7f583d6-0b86-442f-ee94-fbe0396affec","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e38a8c2f-aea2-4b93-b198-4cfc2696b26e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sc"],"metadata":{"id":"NVLDYm1nDz4k","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"a99b5439-1ede-421e-f2d4-1f627f6810c2","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"370ce42f-2455-486a-88c4-f98048cb0998","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# RDDs"],"metadata":{"id":"rhJykridVOvF","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d7700d44-f5ba-49ba-8827-86c3af2142f1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["[**Resilient Distributed Dataset (RDD)**](https://spark.apache.org/docs/latest/rdd-programming-guide.html) is the main data object in Spark and it is an abstraction of the data parallelization. This means that we can work with a single RDD, where in fact its data, as well as its processing, may be distributed in the cluster.\n\nData sharing is slow in MapReduce due to replication, serialization, and disk IO (Actually, most Hadoop applications spend more than 90% of the time doing HDFS read-write operations.). Recognizing this problem, RDDs support **in-memory** processing computation. This means, it stores the state of memory as an object across the jobs and the object is sharable between those jobs.\n\n> **Notes:**\n> * RDDs are immutable, which has a great influence on the appearence of Spark code.\n> * If the elements of an RDD are tuples (which is a Spark data type equivalent to Python tuples of length 2), then each tuple is automatically recognized as a pair of a **key** and a **value**, and we say that it is a *pair RDD*."],"metadata":{"id":"WjXhflutVOvG","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d5ddce69-9b88-4be8-a632-f1482eb5ee67","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Playground"],"metadata":{"id":"WH1jxDkRydAP","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d84a69c5-6baa-4a28-a595-63edd36830f2","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["`SparkContext` has various methods for creating RDDs from various sources, especially within Hadoop, e.g. [newAPIHadoopFile()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext.newAPIHadoopFile), however these are less common due to specific loaders we will see later. For this notebook we will suffice with two RDD creators - `parallelize()` and `textFile()`."],"metadata":{"id":"uiZ3WkVSqtaC","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"389ae180-09ab-4e65-ab9d-57e03560ad75","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### `parallelize()`"],"metadata":{"id":"fXYqquX1gKKA","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5f8dfb2-5d30-463f-addc-4ce41f5649ed","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The [`parallelize()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.parallelize) method takes a *Pythonic* iterable object and creates an RDD with its elements."],"metadata":{"id":"IromLrsALQuf","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d3a1b2e-1778-41b5-ad47-06472e160daa","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["my_list = [12, 23, 34, 45, 56, 67, 78, 89, 90]\nrdd1 = sc.parallelize(my_list)\nprint(type(rdd1))\n# print(rdd1)"],"metadata":{"id":"NAfTAJncHLG9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"953fd7b6-4bb0-4e78-b2de-87eb1810b171","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c0e0cd67-42e9-43a4-afaf-870ab8f27556","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["`rdd1` is of an RDD type, and we can look in the [`RDD` API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis) for all its available methods. See some self-explanatory illustrations below:"],"metadata":{"id":"jwCBkI1CfDYc","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9e6cd8d-d052-4a5d-ba68-4bf28f9bb757","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["rdd2 = rdd1.map(lambda x: x/2)\nrdd2.collect()"],"metadata":{"id":"G1ZBVZJQHS3D","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8aa6e85-e36c-46db-b5b5-9d5d21a3f584","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"52ce62ba-9373-41fe-a4fe-e7567cde0e89","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd3 = rdd1.filter(lambda x: x%2==0)\nrdd3.collect()"],"metadata":{"id":"zqNQpf5UNHox","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b26e71f7-e58f-4078-e331-8957ca0b3fca","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"19153675-0333-424e-8fd5-240e811fa25b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd4 = rdd1.groupBy(lambda x: x%3)\nprint(list(rdd4.collect()))\nprint(list(rdd4.collect()[1][1]))"],"metadata":{"id":"BRzta6Y1NgR3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"67bbe1c0-4e2b-4ad0-c7a6-002ce8eb2bf0","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6ae84ecf-6bf1-446c-938f-b497c6a56b27","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd4.mapValues(lambda iterable: list(iterable)).collect()"],"metadata":{"id":"xAF9n1AnBtbq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d50a1149-7162-4b73-eab2-bdd044e574f9","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04b6d48f-99aa-4ba2-b95c-9eaa1471c762","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd4.map(lambda tup: list(tup[1])).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vt4MMPe5iZ2E","outputId":"c3655530-d5bb-423d-b27f-9bd4ff3731f1","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f391cbd8-2ad3-452c-b950-f82e6a37bc53","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["> **Note:** Defining RDDs with iterators returns a `PipelineRDD` object, which doesn't support the standard RDD methods. Compare with `sc.parallelize(range(100))`."],"metadata":{"id":"jzTyk50-zKux","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10bf4232-bb10-4816-acdd-25012acce4fc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### `textFile()`"],"metadata":{"id":"Iyfxh9S4kQs8","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1971e0aa-67b5-4093-a885-c416cb84dec5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Alternatively, we can create an RDD from a text file using the [`textFile()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile) method. We demonstrate with the text of the book *Moby Dick* (available [here](https://drive.google.com/drive/folders/1NuT5ic0FHCXkoQoQnkBE5e1KVknm8SXP?usp=sharing))"],"metadata":{"id":"ICqq6anPqR9u","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"47524df7-1098-4a33-98cc-4a01b6e67b3d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["text = sc\\\n    .textFile(\"dbfs:/FileStore/texts/melville_moby_dick.txt\")"],"metadata":{"id":"Hs2tPnhshltl","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0fc882c8-192d-41d9-b099-92ca11fef02f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(text)"],"metadata":{"id":"OZl5Bd4ztxso","colab":{"base_uri":"https://localhost:8080/"},"outputId":"da8ca6bb-2a39-472a-b998-50ae0b01363e","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c66553d5-f371-497b-a6bd-2b861373c130","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Like with normal text files, the RDD is made of the lines in the book, which were separated in the file with `\\n`."],"metadata":{"id":"4KpHJXdrvZsB","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c51af74d-6845-4d20-9187-61d070e9ca8e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["text.take(15)"],"metadata":{"id":"PdRDj3HGt0Js","colab":{"base_uri":"https://localhost:8080/"},"outputId":"39fbd958-bc1e-4d56-b642-3d737eff1229","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9bc39f12-d8de-434e-9900-7447f02bd244","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Transformations and actions"],"metadata":{"id":"EOc5hIgqVOvH","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60d4db05-fe6c-4484-82bd-f19a1cbf7ad0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["RDD **transformations** are operations applied on RDDs to yield a new RDD. On the other hand, **actions** are operations applied on RDDs to yield a non-RDD result (number, string, list, etc.). \n\nHere are some examples:\n\n* Transformations:\n    * _map(func)_ - Returns a new distributed dataset, formed by passing each element of the source through a function func.\n    * _filter(func)_ - Returns a new dataset formed by selecting those elements of the source on which func returns true.\n    * _groupByKey()_ - When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable(V)) pairs.\n    * _sortByKey([ascending])_ - When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the Boolean ascending argument.\n* Actions:\n    * _count()_ - Returns the number of elements in the dataset.\n    * _take(n)_ - Returns an array with the first n elements of the dataset. \n    * _saveAsTextFile(path)_ - Writes the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark calls _toString()_ on each element to convert it to a line of text in the file."],"metadata":{"id":"M-z7hXH_VOvI","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c347658-3c52-48a7-8176-afc6b7e68dea","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["> **Warning:** There is an action called [`collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) which is similar to the _take()_ action, but returns **all** the elements of the RDD. This action collects the relevant elements to the master of the cluster, and can easily crush the system. This is why it is usually useful after a filter or other operation that returns a sufficiently small subset of the data."],"metadata":{"id":"3c5FlY6_WXIG","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b763a525-80bd-45c9-b954-e8296d1a1c8c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["> **Note:** In most cases one applies a chain of transformations which ends with an action. Each RDD in such dependency chain has a pointer (dependency) to its parent RDD. Spark is **lazy**, so nothing will be executed until an action will trigger the chain. Therefore, RDD transformation is not a set of data but is a step in a program (might be the only step) telling Spark how to get data and what to do with it.\n>\n> **Reference:** [Explanantion about lazy calculation by DataFlair](https://data-flair.training/blogs/apache-spark-lazy-evaluation/)"],"metadata":{"id":"7z61lDZHXmi4","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58805068-6c23-4697-8129-085529b74135","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Spark SQL & DataFrames"],"metadata":{"id":"ZMYMlAELWXIH","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7d6f269c-7ecc-40f4-b8ac-a3b2f3f8df81","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including plain SQL and the **Dataframe** API. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation."],"metadata":{"id":"ElOFoORzWXII","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ddf2b6d4-cb2f-49a8-a8eb-870bae60a64d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Playground"],"metadata":{"id":"mTt2xfZ7GQCq","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9866c6fb-defe-473b-918e-7223850b3435","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["A `SparkContext` is what you need for working with the Spark core elements, however the entry point into Spark SQL functionality is the `SparkSession` object (traditionally symbolized as the variable `spark`)."],"metadata":{"id":"TijcKQtqGS49","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90f263ea-b15c-4081-baa8-a2d821c01120","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# from pyspark.sql import SparkSession\n# spark = SparkSession.builder.getOrCreate()"],"metadata":{"id":"IKYqvz4PGlFT","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9a8412b-57c9-4ba2-be5a-dbd789756f8c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark"],"metadata":{"id":"tX4fI2IeGU1k","colab":{"base_uri":"https://localhost:8080/","height":219},"outputId":"aa794ed5-dc01-4af1-b3f2-47324cd47d7a","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2e22b88f-ce1a-44d4-bc6b-e080f72db3db","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["`spark` has many useful types of [`DataFrameReader`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader), which upload *structured* data from various sources (e.g. relational databases, Kafka topics, etc.) into Spark DataFrames. In this notebook we will focus on the most simple reader - CSV reader."],"metadata":{"id":"GCaXdgkmGsDf","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f4c541bd-0c7d-4ebe-9286-5b976d800ab2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dessert = spark.read.csv(\"/FileStore/tables/dessert.csv\", \n                         header=True, \n                         inferSchema=True)"],"metadata":{"id":"hCC0SgmXHSvL","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"583321d1-95c7-48ec-ba60-bcc537b7f8ae","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(dessert)"],"metadata":{"id":"-S8e7VVJGvAY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"80167052-a1aa-4401-f888-6c6262295717","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8d030bef-3467-433f-a816-b44b00da3e40","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["> **Note:** We use the default `inferSchema=True`, but you can also specify your own schema when reading the data (or before). To do that you need to use [`StructType`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType), as explained in [this introduction](https://mungingdata.com/apache-spark/dataframe-schema-structfield-structtype/) by Munging Data, and will be coverred later in the course."],"metadata":{"id":"s80TUxn9zRRm","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d8070150-cee0-44ff-954d-838388bf9b1d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dessert.show(5)"],"metadata":{"id":"XpVM5aGNHg3r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a95055f-19ea-4f83-a8ca-77833607d51a","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"aa840aba-8c1b-4489-be9c-3b85577fad65","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(dessert)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"37ed0905-f349-4f07-8f4d-15a2b4a1b520","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Each record in the \"dessert\" dataset describes a group visit at a restaurant. We demonstrate below some self-explanatory DataFrame manipulations (see [DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html) for more details)."],"metadata":{"id":"9qqw9GHbVBTR","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d83713c9-5b41-4cb8-b229-07ef13fcb061","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dessert.printSchema()"],"metadata":{"id":"WqIYgTyOxOiy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"41f38818-bdba-41db-c434-7e826440cbe8","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"411cf2c0-112d-49ed-99b9-72197f7f52e1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dessert = dessert\\\n    .withColumnRenamed('dessert', 'purchase')\\\n    .withColumnRenamed('day.of.week', 'weekday')\\\n    .withColumnRenamed('num.of.guests', 'guests')\ndessert.show(5)"],"metadata":{"id":"tqvlcCsKrP_2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"550ca152-7094-472f-debc-67ec2dc57618","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e89129c7-ee8b-404c-8e28-303760764756","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dessert = dessert\\\n    .withColumn('no_purchase', ~dessert.purchase)\ndessert.show(5)"],"metadata":{"id":"HH8wOBbrpMPq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5c88baf-84a9-469f-e841-cd348cfd1993","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8e716bf6-e42e-477a-9bdc-23ee13a7eea5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dessert = dessert\\\n    .withColumn(\"purchase\", dessert[\"purchase\"].cast(\"integer\"))\\\n    .withColumn(\"no_purchase\", dessert[\"no_purchase\"].cast(\"integer\"))\ndessert.show(5)"],"metadata":{"id":"1XrWeaELs3RO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"38becca7-ba7e-45c0-eb00-ba3080304837","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"da0c5c02-7897-47c1-b6e9-2c1e5425d239","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f20fa7d0-5689-495d-831d-d140ba6091c6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["buyers = dessert\\\n    .groupBy('guests')\\\n    .agg(f.sum(dessert.purchase).alias('buyers'), \n         f.sum(dessert.no_purchase).alias('non_buyers'))\nbuyers.show()"],"metadata":{"id":"2Ku9T0BftGiU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f601451-c586-4ff9-e022-2b8736369307","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"26570984-c35f-4635-8890-fab20978259b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["buyers.toPandas().set_index('guests').sort_index().plot.bar()"],"metadata":{"id":"M209-y0rr8Zk","colab":{"base_uri":"https://localhost:8080/","height":299},"outputId":"158f22ab-1151-421d-eb86-18cd4d0b1455","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bf288d48-394e-4f41-840d-bc84bab3eed9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Transformation and actions"],"metadata":{"id":"ycxmcGZSWXIK","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c35c81f-0dbd-42a3-96c7-8142097c7bcf","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Dataframe is a special type of RDD, and as such it supports all RDD operations (as an RDD of `Row` elements) and an additional large set of unique attributes and methods:\n\n* Attributes\n    * `column` - Returns all column names as a list.\n    * `rdd` - Returns the content as an RDD (of `Row` elements).\n    * `schema` - Returns the schema of this DataFrame (as a `StructType`).\n* Methods\n    * `crosstab(col1, col2)` - Computes a pair-wise frequency table of the given columns (pivot table).\n    * `drop(*cols)` - Returns a new DataFrame that drops the specified column.\n    * `head(n=None)` - Returns the first n rows.\n    * `orderBy(*cols)` - Returns a new DataFrame sorted by the specified column(s).\n    * `printSchema()` - Prints out the schema in a tree format.\n    * `where(condition)` - an alias for `filter()`"],"metadata":{"id":"2C2nzZXIWXIL","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"133bf307-bbda-4599-818c-7a3002151bae","inputWidgets":{},"title":""}}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"application/vnd.databricks.v1+notebook":{"notebookName":"03 - Introduction to Spark and PySpark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
